# -*- coding: utf-8 -*-
"""Data_pipeline_json.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JgQHlUrIk3N2CrE2FuzCpQgp5fjPx1Ob
"""

# -*- coding: utf-8 -*-
"""
Data Pipeline: Extract → Clean → Chunk → Generate Real Q&A → Save JSON + Cleaned Text
Only REAL Q&A | JSON format with supporting_passages | Cleaned text: 1 sentence per line
"""

# =============================================
# STEP 0: SETUP (Run once)
# =============================================
!pip install -q PyPDF2 transformers torch sentence-transformers pandas tqdm nltk

import PyPDF2
import re
import json
import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from tqdm import tqdm
from google.colab import files
import nltk

nltk.download('punkt_tab', quiet=True)
from nltk.tokenize import sent_tokenize

# Clear GPU cache
torch.cuda.empty_cache()
print("Setup complete!")

# =============================================
# STEP 1: UPLOAD YOUR PDF
# =============================================
uploaded = files.upload()  # Upload your bank_policies.pdf
pdf_path = list(uploaded.keys())[0]
print(f"Uploaded: {pdf_path}")

CHUNK_SIZE          = 500       # words per chunk
OVERLAP             = 100
QUESTIONS_PER_CHUNK = 7
MAX_NEW_TOKENS      = 350

def is_file_pdf(name):  return os.path.splitext(name)[1].lower() == '.pdf'
def is_file_txt(name):  return os.path.splitext(name)[1].lower() == '.txt'

# =============================================
# STEP 2: EXTRACT TEXT FROM PDF/TXT
# =============================================
def extract_text_from_pdf(file_path):
    if is_file_pdf(file_path):
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            return "\n".join(page.extract_text() or "" for page in reader.pages)
    elif is_file_txt(file_path):
        return open(file_path, "r", encoding="utf-8").read()
    else:
        raise ValueError(f"Unsupported file type: {file_path}")

raw_text = extract_text_from_pdf(pdf_path)
print(f"Extracted {len(raw_text.split())} words ({len(raw_text)} chars).")

# =============================================
# STEP 3: CLEAN TEXT
# =============================================
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Collapse whitespace
    text = re.sub(r'[^a-zA-Z0-9.,;:!?()$%\- ]', '', text)  # Keep safe chars
    return text.strip()

cleaned_text = clean_text(raw_text)
print(f"Cleaned text: {len(cleaned_text.split())} words")

# =============================================
# STEP 4: SAVE CLEANED TEXT – ONE SENTENCE PER LINE
# =============================================
def save_cleaned_text_one_sentence_per_line(text, filename="cleaned_bank_corpus.txt"):
    """Save cleaned text with one sentence per line."""
    sentences = sent_tokenize(text)
    with open(filename, "w", encoding="utf-8") as f:
        for sent in sentences:
            sent = sent.strip()
            if sent:
                f.write(sent + "\n")
    print(f"Cleaned corpus saved → {filename} ({len(sentences)} sentences)")

save_cleaned_text_one_sentence_per_line(cleaned_text)

# =============================================
# STEP 5: CHUNK TEXT
# =============================================
def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

chunks = chunk_text(cleaned_text, chunk_size=CHUNK_SIZE, overlap=OVERLAP)
print(f"Created {len(chunks)} chunks ({CHUNK_SIZE} words, {OVERLAP} overlap)")

# =============================================
# STEP 6: LOAD Phi-2 FOR Q/A GENERATION
# =============================================
phi_model_name = "microsoft/phi-2"
phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)
phi_model = AutoModelForCausalLM.from_pretrained(
    phi_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# qa_pipeline = pipeline(
#     "text-generation",
#     model=phi_model,
#     tokenizer=phi_tokenizer,
#     max_new_tokens=MAX_NEW_TOKENS,
#     temperature=0.3,
#     top_p=0.92,
#     do_sample=True,
#     pad_token_id=phi_tokenizer.eos_token_id,
#     return_full_text=False,            # <-- ONLY the new text
#     eos_token_id=phi_tokenizer.eos_token_id,
#     # force the model to stop only at </s> or a blank line
#     stopping_criteria=[
#         lambda input_ids, scores: input_ids[-1] == phi_tokenizer.eos_token_id
#     ]
# )
# qa_pipeline.tokenizer.pad_token = qa_pipeline.tokenizer.eos_token
phi_tokenizer.pad_token = phi_tokenizer.eos_token
phi_model.config.pad_token_id = phi_tokenizer.eos_token_id
print("Phi-2 loaded for Q&A generation")

# =============================================
# STEP 7: BUILD PROMPT & GENERATE Q&A
# =============================================
# def build_prompt(chunk):
#     example = """
# Example:
# Passage: A dormant account has no transactions for 12 months.
# Q1: What is a dormant account?
# A1: An account is classified as dormant if there are no customer-initiated transactions for a continuous period of twelve months.
# """
#     return f"""You are a banking expert. Generate EXACTLY 7 Q&A pairs from the passage below.

# {example}

# Passage:
# {chunk.strip()}

# OUTPUT ONLY THE 7 PAIRS IN THIS FORMAT (no extra text, no explanations):

# Q1: <question>?
# A1: <answer>

# Q2: <question>?
# A2: <answer>

# ...

# Q7: <question>?
# A7: <answer>
# """



def build_prompt(chunk):
    example = """
Example:
Passage: The Dormant Account Policy (Policy 12.5) ensures accounts with no activity for 12 months are flagged.
Q1: What is the inactivity period in Dormant Account Policy (Policy 12.5)?
A1: The policy flags accounts dormant after 12 months of no customer-initiated transactions.
"""
    return f"""You are a banking expert creating precise Q&A for training.

{example}

From the passage below, generate EXACTLY 7 specific Q&A pairs.

Passage:
{chunk.strip()}

INSTRUCTIONS:
- Questions MUST be specific: include policy name, section, number, or exact term (e.g., "What is the fee in Overdraft Policy (Section 4.2)?").
- AVOID vague questions like "What is the purpose of this policy?" or "What does this mean?".
- Answers: detailed, direct from passage, 1-3 sentences.
- Use exact numbers, terms, conditions.
- Format exactly:

Q1: [specific question]?
A1: [detailed answer]

... up to Q7.
No extra text.
"""

def generate_qa_from_chunk(chunk):
    prompt = build_prompt(chunk)

    # Tokenize
    inputs = phi_tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    ).to(phi_model.device)

    try:
        # Direct model call — NO pipeline, NO caching, NO batching
        with torch.no_grad():
            output_ids = phi_model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.3,
                top_p=0.92,
                do_sample=True,
                pad_token_id=phi_tokenizer.eos_token_id,
                eos_token_id=phi_tokenizer.eos_token_id
            )

        # Decode only the new part
        generated = phi_tokenizer.decode(
            output_ids[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()

        if not generated:
            print(f"Warning: Empty output for chunk {len(qa_pairs)}")

        return generated

    except Exception as e:
        print(f"Generation error: {e}")
        return ""

# =============================================
# STEP 8: GENERATE Q&A FROM CHUNKS
# =============================================
qa_pairs = []
print("Generating Q&A pairs from chunks...")
for i, chunk in enumerate(tqdm(chunks[:15])):  # Remove [:5] to process all
    raw_qa = generate_qa_from_chunk(chunk)
    # print("this is the raw_qa", raw_qa)
    if raw_qa:
        qa_pairs.append({"chunk_id": i, "chunk_text": chunk, "raw_qa": raw_qa})

print(f"Generated {len(qa_pairs)} raw Q&A blocks")

# =============================================
# STEP 9: PARSE Q&A INTO STRUCTURED FORMAT
# =============================================
def parse_qa_block(block):
    lines = [line.strip() for line in block.split('\n') if line.strip()]
    questions, answers = [], []
    i = 0
    while i < len(lines) - 1:
        q_line = lines[i]
        a_line = lines[i + 1]
                # Relaxed matching: allow spaces, dots, missing ?, case-insensitive
        q_match = re.match(r'^\s*Q\s*(\d+)[\s:.-]*([^?]*)(\?)?\s*$', q_line, re.IGNORECASE)
        a_match = re.match(r'^\s*A\s*(\d+)[\s:.-]*\s*(.+)$', a_line, re.IGNORECASE)
        if q_match and a_match:
            q_num = q_match.group(1)
            a_num = a_match.group(1)
            if q_num == a_num:
                question = q_match.group(2).strip()
                answer = a_match.group(2).strip()
                if question and answer:  # Ensure non-blank
                    questions.append(question)
                    answers.append(answer)
                i += 2
                continue
        i += 1
    return list(zip(questions, answers))


# Build final JSON dataset
final_dataset = []
for item in qa_pairs:
    parsed = parse_qa_block(item["raw_qa"])
    for q, a in parsed:
        final_dataset.append({
            "question": q,
            "answer": a,
            "supporting_passages": [item["chunk_text"]]  # Original chunk as evidence
        })

print(f"Parsed {len(final_dataset)} real Q&A pairs with supporting passages")

redundancy = False ## changes
example_question_redundancy1 = "What is a dormant account?"
example_question_redundancy2 = "What is a dormant account"
# After building final_dataset
cleaned_dataset = []
for item in final_dataset:
    q = item["question"].strip()
    a = item["answer"].strip()
    # Remove entries with placeholders
    if "<question>" in q or "<answer>" in a or not q or not a:
        continue
    elif example_question_redundancy1 in q or example_question_redundancy2 in q:
        if redundancy:
            continue
        else:
            redundancy = True
    cleaned_dataset.append(item)

final_dataset = cleaned_dataset
print(f"After cleaning: {len(final_dataset)} valid Q&A pairs")

# =============================================
# STEP 10: SAVE TO JSON (List of dictionaries)
# =============================================
json_output_file = "bank_qa_dataset.json"

with open(json_output_file, "w", encoding="utf-8") as f:
    json.dump(final_dataset, f, indent=2, ensure_ascii=False)

print(f"Saved {len(final_dataset)} real Q&A records → {json_output_file}")

# Optional: Show first 2 entries
print("\nSample JSON entries:")
for entry in final_dataset[:2]:
    print(json.dumps(entry, indent=2))

# =============================================
# STEP 11: DOWNLOAD FILES
# =============================================
files.download(json_output_file)
files.download("cleaned_bank_corpus.txt")

print("\nPipeline complete!")
print(f"→ {json_output_file} (real Q&A with supporting passages)")
print(f"→ cleaned_bank_corpus.txt (1 sentence per line)")

