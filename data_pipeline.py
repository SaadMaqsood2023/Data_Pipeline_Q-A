# -*- coding: utf-8 -*-
"""Data_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1caLzOUN4Nlm-_dfDZV6aSRGDE9rY_d

## Data Pipeline

Overall pipeline flow
"""

# =============================================
# STEP 0: SETUP (Run once)
# =============================================
!pip install -q PyPDF2 transformers torch sentence-transformers faiss-cpu pandas tqdm

import PyPDF2
import re
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from tqdm import tqdm
import os

# Clear GPU cache
torch.cuda.empty_cache()

print("Setup complete!")

# =============================================
# STEP 1: UPLOAD YOUR PDF
# =============================================
from google.colab import files
uploaded = files.upload()  # Upload your bank_policies.pdf

pdf_path = list(uploaded.keys())[0]
print(f"Uploaded: {pdf_path}")

def is_file_pdf(file_name):
  return os.path.splitext(file_name)[1].lower() == '.pdf'

def is_file_txt(file_name):
  return os.path.splitext(file_name)[1].lower() == '.txt'

# =============================================
# STEP 2: EXTRACT TEXT FROM PDF
# =============================================


def extract_text_from_pdf(file_path):
    text = ""
    if is_file_pdf(pdf_path):
      with open(file_path, "rb") as file:
          reader = PyPDF2.PdfReader(file)
          for page in reader.pages:
              page_text = page.extract_text()
              if page_text:
                  text += page_text + "\n"
      return text
    elif is_file_txt(pdf_path):
      with open(file_path, "r") as file:
          text = file.read()
      return text
    else:
      raise ValueError(f"Unsupported file type: {file_path}")
raw_text = extract_text_from_pdf(pdf_path)
print(f"Extracted {len(raw_text.split())} words from {len(raw_text)} characters.")

# =============================================
# STEP 3: CLEAN & CHUNK TEXT (200 words, 50 overlap)
# =============================================
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9.,;:!?()$%\- ]', '', text)  # Keep only safe chars
    return text.strip()

def chunk_text(text, chunk_size=200, overlap=50):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

cleaned_text = clean_text(raw_text)
chunks = chunk_text(cleaned_text, chunk_size=200, overlap=50)

print(f"Created {len(chunks)} chunks (200 words each, 50 overlap)")
print(f"Example chunk:\n{chunks[10][:500]}...")

# =============================================
# STEP 4: LOAD Phi-2 FOR Q/A GENERATION
# =============================================
phi_model_name = "microsoft/phi-2"
phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)
phi_model = AutoModelForCausalLM.from_pretrained(
    phi_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

qa_pipeline = pipeline(
    "text-generation",
    model=phi_model,
    tokenizer=phi_tokenizer,
    max_new_tokens=80,
    temperature=0.3,
    do_sample=True,
    pad_token_id=phi_tokenizer.eos_token_id
)

print("Phi-2 loaded for Q/A generation")

# =============================================
# STEP 5: GENERATE 4 Q/A PER CHUNK
# =============================================
def generate_qa_from_chunk(chunk):
    prompt = f"""Given the following bank policy text, generate 4 short, factual question-answer pairs.
Each answer must be under 25 words and cite the policy if possible.

Text: {chunk}

Format:
Q1: [question]?
A1: [answer]
Q2: ...
"""

    try:
        full_output = qa_pipeline(prompt, max_new_tokens=200)[0]['generated_text']

        # Extract only the part AFTER the original chunk
        if "Text:" in full_output:
            generated = full_output.split("Text:", 1)[1]
        else:
            generated = full_output[len(prompt):]  # fallback

        return generated.strip()
    except Exception as e:
        print(f"Error: {e}")
        return ""




qa_pairs = []
print("Generating Q/A pairs from chunks...")

for i, chunk in enumerate(tqdm(chunks[:5])):  # LIMIT TO 100 CHUNKS FOR NOW
    raw = generate_qa_from_chunk(chunk)
    print("chunk id is ", i, "And chunk is ", chunk)
    print("raw is", raw)
    qa_pairs.append({"chunk_id": i, "raw_qa": raw})

print(f"Generated {len(qa_pairs)} raw Q/A blocks")

print(type(chunks))
# print(chunks)
print(qa_pairs[0])

# =============================================
# STEP 6: PARSE Q/A INTO CLEAN LIST
# =============================================
def parse_qa_block(block):
    lines = [line.strip() for line in block.split('\n') if line.strip()]
    questions = []
    answers = []
    i = 0
    while i < len(lines):
        if lines[i].startswith("Q") and lines[i].endswith("?"):
            q = lines[i][3:].strip()  # Remove "Q1: "
            if i+1 < len(lines) and lines[i+1].startswith("A"):
                a = lines[i+1][3:].strip()
                questions.append(q)
                answers.append(a)
                i += 2
            else:
                i += 1
        else:
            i += 1
    return list(zip(questions, answers))

all_qa = []
for item in qa_pairs:
    parsed = parse_qa_block(item["raw_qa"])
    for q, a in parsed:
        all_qa.append({"question": q, "answer": a, "source": "real", "chunk_id": item["chunk_id"]})

real_df = pd.DataFrame(all_qa)
print(f"Parsed {len(real_df)} clean Q/A pairs")
real_df.head()

# =============================================
# STEP 7: GENERATE FAKE ANSWERS (Using Llama-70B 4-bit)
# =============================================
# !pip install bitsandbytes accelerate

# from transformers import AutoModelForCausalLM as LlamaModel, AutoTokenizer

# llama_model_name = "meta-llama/Meta-Llama-3.1-70B-Instruct"
# llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)

# llama_model = LlamaModel.from_pretrained(
#     llama_model_name,
#     device_map="auto",
#     load_in_4bit=True,
#     torch_dtype=torch.float16
# )

# fake_generator = pipeline(
#     "text-generation",
#     model=llama_model,
#     tokenizer=llama_tokenizer,
#     max_new_tokens=60,
#     temperature=0.9,
#     do_sample=True
# )

fake_generator = qa_pipeline

print("Generating fake answers...")

fake_qa = []
for _, row in tqdm(real_df.iterrows(), total=len(real_df)):
    prompt = f"Answer this bank policy question incorrectly or with made-up info:\nQ: {row['question']}\nA:"
    try:
        fake = fake_generator(prompt)[0]['generated_text'].split("A:")[-1].strip()
        fake_qa.append({
            "question": row["question"],
            "answer": fake,
            "source": "fake",
            "chunk_id": row["chunk_id"]
        })
    except:
        continue

fake_df = pd.DataFrame(fake_qa)
print(f"Generated {len(fake_df)} fake answers")

# =============================================
# STEP 8: COMBINE & SAVE FINAL DATASET
# =============================================
# Combine real + fake
discriminator_data = pd.concat([real_df, fake_df], ignore_index=True)
discriminator_data["label"] = discriminator_data["source"].map({"real": 1, "fake": 0})

# Shuffle
discriminator_data = discriminator_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Save
output_file = "bank_discriminator_dataset.csv"
discriminator_data.to_csv(output_file, index=False)
print(f"Saved {len(discriminator_data)} samples â†’ {output_file}")

# Show sample
discriminator_data.head(10)

# =============================================
# STEP 9: DOWNLOAD YOUR DATASET
# =============================================
from google.colab import files
files.download(output_file)

