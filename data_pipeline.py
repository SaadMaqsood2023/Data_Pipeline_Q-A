# -*- coding: utf-8 -*-
"""Data_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1caLzOUN4Nlm-_dfDZV6aSRGDE9rY_d

## Data Pipeline

Overall pipeline flow
"""

# =============================================
# STEP 0: SETUP (Run once)
# =============================================
!pip install -q PyPDF2 transformers torch sentence-transformers faiss-cpu pandas tqdm

import PyPDF2
import re
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from tqdm import tqdm
import os

# Clear GPU cache
torch.cuda.empty_cache()

print("Setup complete!")

# =============================================
# STEP 1: UPLOAD YOUR PDF
# =============================================
from google.colab import files
uploaded = files.upload()  # Upload your bank_policies.pdf

pdf_path = list(uploaded.keys())[0]
print(f"Uploaded: {pdf_path}")

CHUNK_SIZE = 500       # ~1200 words
OVERLAP = 100
QUESTIONS_PER_CHUNK = 7 # generate 7 Q&A per chunk
MAX_NEW_TOKENS = 300

def is_file_pdf(file_name):
  return os.path.splitext(file_name)[1].lower() == '.pdf'

def is_file_txt(file_name):
  return os.path.splitext(file_name)[1].lower() == '.txt'

# =============================================
# STEP 2: EXTRACT TEXT FROM PDF
# =============================================


def extract_text_from_pdf(file_path):
    text = ""
    if is_file_pdf(pdf_path):
      with open(file_path, "rb") as file:
          reader = PyPDF2.PdfReader(file)
          for page in reader.pages:
              page_text = page.extract_text()
              if page_text:
                  text += page_text + "\n"
      return text
    elif is_file_txt(pdf_path):
      with open(file_path, "r") as file:
          text = file.read()
      return text
    else:
      raise ValueError(f"Unsupported file type: {file_path}")
raw_text = extract_text_from_pdf(pdf_path)
print(f"Extracted {len(raw_text.split())} words from {len(raw_text)} characters.")

# =============================================
# STEP 3: CLEAN & CHUNK TEXT (200 words, 50 overlap)
# =============================================
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9.,;:!?()$%\- ]', '', text)  # Keep only safe chars
    return text.strip()

def chunk_text(text, chunk_size=200, overlap=50):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

cleaned_text = clean_text(raw_text)
chunks = chunk_text(cleaned_text, chunk_size=CHUNK_SIZE, overlap=OVERLAP)

print(f"Created {len(chunks)} chunks (1000 words each, 150 overlap)")
print(f"Example chunk:\n{chunks[10][:500]}...")
print(f"Datatype of chunks is: {type(chunks)}")

# =============================================
# STEP 4: LOAD Phi-2 FOR Q/A GENERATION
# =============================================
phi_model_name = "microsoft/phi-2"
phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)
phi_model = AutoModelForCausalLM.from_pretrained(
    phi_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

qa_pipeline = pipeline(
    "text-generation",
    model=phi_model,
    tokenizer=phi_tokenizer,
    max_new_tokens=MAX_NEW_TOKENS,
    temperature=0.3,
    top_p=0.92,
    do_sample=True,
    pad_token_id=phi_tokenizer.eos_token_id
)

print("Phi-2 loaded for Q/A generation")

# =============================================
# STEP 5: GENERATE 4 Q/A PER CHUNK
# =============================================
def build_prompt(chunk):
    example_qa = """
Example:
Passage: A dormant account has no transactions for 12 months.
Q: What is a dormant account?
A: An account is classified as dormant if there are no customer-initiated transactions (such as deposits, withdrawals, or transfers) for a continuous period of twelve months. Bank-initiated actions like interest credits or fee deductions do not count toward activity.
"""

    return f"""You are a banking expert creating high-quality training data.

{example_qa}

Now, from this passage, generate exactly 7 detailed Q&A pairs:

Passage:
{chunk.strip()}

INSTRUCTIONS:
- Generate exactly 7 question-answer pairs.
- Each question must be specific, clear, and test a unique policy detail, rule, or definition.
- Each answer must be a complete, detailed sentence (or 2–3 sentences if needed) directly based on the passage.
- Answers should be informative and precise, not just short phrases.
- Use real terms, numbers, time periods, and conditions from the passage.
- Avoid giving answers just in 1 or 2 words

Format exactly:
Q1: [question]?
A1: [detailed answer]

Q2: [question]?
A2: [detailed answer]

... up to Q7: and A7:
Generate 7 pairs only. No explanations, no extra text.
"""

def generate_qa_from_chunk(chunk):
    prompt = build_prompt(chunk)
    try:
        full_output = qa_pipeline(prompt, max_new_tokens=MAX_NEW_TOKENS)[0]['generated_text']
        # Extract only the part AFTER the original chunk
        if "Text:" in full_output:
            generated = full_output.split("Text:", 1)[1]
        else:
            generated = full_output[len(prompt):]  # fallback

        return generated.strip()
    except Exception as e:
        print(f"Error: {e}")
        return ""


qa_pairs = []
print("Generating Q/A pairs from chunks...")

for i, chunk in enumerate(tqdm(chunks[:5])):  # LIMIT TO 100 CHUNKS FOR NOW
    raw = generate_qa_from_chunk(chunk)
    qa_pairs.append({"chunk_id": i, "raw_qa": raw})

print(f"Generated {len(qa_pairs)} raw Q/A blocks")

# =============================================
# STEP 6: PARSE Q/A INTO CLEAN LIST
# =============================================
def parse_qa_block(block):
    lines = [line.strip() for line in block.split('\n') if line.strip()]
    questions = []
    answers = []
    i = 0
    while i < len(lines) - 1:
        q_line = lines[i]
        a_line = lines[i + 1]

        # Match Q1:, Q2:, ..., Q7:
        if re.match(r'^Q\d+:', q_line) and q_line.endswith('?'):
            # Match A1:, A2:, etc.
            if re.match(r'^A\d+:', a_line):
                q_num = re.search(r'Q(\d+):', q_line).group(1)
                a_num = re.search(r'A(\d+):', a_line).group(1)

                # Ensure Q and A numbers match
                if q_num == a_num:
                    question = q_line.split(':', 1)[1].strip()  # After "Q1:"
                    answer = a_line.split(':', 1)[1].strip()    # After "A1:"
                    questions.append(question)
                    answers.append(answer)
                    i += 2
                else:
                    i += 1  # Skip mismatched
            else:
                i += 1
        else:
            i += 1

    return list(zip(questions, answers))

all_qa = []
for item in qa_pairs:
    parsed = parse_qa_block(item["raw_qa"])
    for q, a in parsed:
        all_qa.append({"question": q, "answer": a, "source": "real", "chunk_id": item["chunk_id"]})

real_df = pd.DataFrame(all_qa)
print(f"Parsed {len(real_df)} clean Q/A pairs")
real_df.head()

# =============================================
# STEP 7: GENERATE FAKE ANSWERS (Using Llama-70B 4-bit)
# =============================================

fake_generator = qa_pipeline

print("Generating fake answers...")

fake_qa = []
for _, row in tqdm(real_df.iterrows(), total=len(real_df)):
    fake_prompt = f"""You are a banking assistant who confidently gives incorrect policy information.
    Answer the question below with a detailed, professional, and believable response — but the facts must be wrong or invented.

    Important:
    - Sound authoritative and use banking terms.
    - Include plausible numbers, time periods, or rules.
    - Do NOT say "I don't know", leave it blank, or repeat the question.
    - Do NOT use placeholders like [], {{ }}, or "answer here".
    - The answer must look 100% real to a customer.

    Q: {row['question']}
    A:"""
    try:
        prompt = fake_prompt
        fake = fake_generator(prompt)[0]['generated_text'].split("A:")[-1].strip()
        fake_qa.append({
            "question": row["question"],
            "answer": fake,
            "source": "fake",
            "chunk_id": row["chunk_id"]
        })
    except:
        continue

fake_df = pd.DataFrame(fake_qa)
print(f"Generated {len(fake_df)} fake answers")

# =============================================
# STEP 8: COMBINE & SAVE FINAL DATASET
# =============================================
# Combine real + fake
discriminator_data = pd.concat([real_df, fake_df], ignore_index=True)
discriminator_data["label"] = discriminator_data["source"].map({"real": 1, "fake": 0})

# Shuffle
discriminator_data = discriminator_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Save
output_file = "bank_discriminator_dataset.csv"
discriminator_data.to_csv(output_file, index=False)
print(f"Saved {len(discriminator_data)} samples → {output_file}")

# Show sample
discriminator_data.head(10)

# =============================================
# STEP 9: DOWNLOAD YOUR DATASET
# =============================================
from google.colab import files
files.download(output_file)

